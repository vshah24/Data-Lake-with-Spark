{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Import all libraries\n",
    "import configparser                                                                                     # To read the cfg file and extract the credentials from there\n",
    "from datetime import datetime                                                                           #Will be useful when we try to convert the ts into datetime\n",
    "import os                                                                                               #What is this for?????\n",
    "from pyspark.sql import SparkSession                                                                    #Useful in Creating a spark session\n",
    "from pyspark.sql.functions import udf, col                                                         #Importing few functions\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read from the config file. Your credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Initiate the spark session\n",
    "spark=SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.1. Read the song_data file. (Read all json file from 1 folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Path of the s3 bucket that holds song data\n",
    "song_data_path=\"s3a://udacity-dend/song_data/A/A/A/*.json\"\n",
    "\n",
    "#Reading from the s3 bucket\n",
    "df=spark.read.json(song_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1.1.a. Retrieve song_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Fetching the columns required for song_table \n",
    "songs_table = df.select(\"song_id\",\"title\",\"artist_id\",\"artist_name\",\"year\",\"duration\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1.1.b.a Writing the song_table back to parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Writing the songs_table in parquet currently our local folder and not on s3. Partitoning them too\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_name\").parquet(\"song1.parquet\")    #Overwrite helps us to copy the folder if it already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1.1.b.b. Write into s3 bucket as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken to write back song_table to s3 is : 224.30409908294678\n"
     ]
    }
   ],
   "source": [
    "output_data=\"s3a://datalake-spark-project/\"\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_name\").parquet(output_data+\"songs/\")    #Overwrite helps us to copy the folder if it already exists\n",
    "end = time.time()\n",
    "print('The time taken to write back song_table to s3 is :',(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1.1.c We can read the parquet file. Will be useful if needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We can read the parquet file too\n",
    "\n",
    "#Local\n",
    "localdf=spark.read.parquet(\"song1.parquet\")\n",
    "\n",
    "\n",
    "#From S3\n",
    "s3df=spark.read.parquet(output_data+ \"songs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1.2. Read Artist Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table = df.select(\"artist_id\",col(\"artist_name\").alias(\"name\"),\n",
    "                          col(\"artist_location\").alias(\"location\"),\n",
    "                          col(\"artist_longitude\").alias(\"longitude\"),\n",
    "                          col(\"artist_latitude\").alias(\"latitude\")) \\\n",
    ".dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+---------------+---------+--------+\n",
      "|         artist_id|         name|       location|longitude|latitude|\n",
      "+------------------+-------------+---------------+---------+--------+\n",
      "|ARSVTNL1187B992A91|Jonathan King|London, England| -0.12714|51.50632|\n",
      "|ARXR32B1187FB57099|          Gob|               |     null|    null|\n",
      "+------------------+-------------+---------------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1. Read the log_data file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Question here arises that how to read all the files after log_data\n",
    "log_data_path=\"s3a://udacity-dend/log_data/2018/11/2018-11-13-events.json\"\n",
    "\n",
    "df=spark.read.json(log_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.1.a. Filter By Page=\"NextSong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page==\"NextSong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.1.b. Retrieve the user table from log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     54|     Kaleb|     Cook|     M| free|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     55|    Martin|  Johnson|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract columns for users table    \n",
    "user_table = df.select(col(\"userId\").alias(\"user_id\"),\n",
    "                       col(\"firstname\").alias(\"first_name\"),\n",
    "                       col(\"lastname\").alias(\"last_name\"),\n",
    "                       \"gender\",\n",
    "                       \"level\").dropDuplicates()\n",
    "\n",
    "user_table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2. Convert the column ts to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We observe the ts column is a long. we will try to convert that to datetime\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.2.a. Convert the ts to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create a timestamp column from ts(epoch)\n",
    "\n",
    "#Import the type timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "#Create a udf function that converts the epoch to timestamp\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000),TimestampType())         #Mention timestamp() or else it will return a function in that column         \n",
    "df = df.withColumn(\"timestamp\",get_timestamp(df.ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.2.b. Retrieve table time. Fetch the month,hour,week column from the newly created column timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+----+-----+----+\n",
      "|          start_time|hour|day|week|month|year|\n",
      "+--------------------+----+---+----+-----+----+\n",
      "|2018-11-13 00:40:...|   0|Tue|  46|   11|2018|\n",
      "|2018-11-13 01:12:...|   1|Tue|  46|   11|2018|\n",
      "|2018-11-13 03:19:...|   3|Tue|  46|   11|2018|\n",
      "+--------------------+----+---+----+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fetch the hour,month,year fro timestamp column that we just created above \n",
    "import pyspark.sql.functions as F\n",
    "time_table = df.select(col(\"timestamp\").alias(\"start_time\"),\n",
    "                   F.hour(\"timestamp\").alias(\"hour\"),\n",
    "                   date_format(col(\"timestamp\"),\"E\").alias(\"day\"),\n",
    "                   F.weekofyear(\"timestamp\").alias(\"week\"),\n",
    "                   F.month(\"timestamp\").alias(\"month\"),\n",
    "                   F.year(\"timestamp\").alias(\"year\")\n",
    "                  )\n",
    "\n",
    "time_table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3. Retrieve songplays table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+--------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|         artist_name|year| duration|\n",
      "+------------------+--------------------+------------------+--------------------+----+---------+\n",
      "|SOEKAZG12AB018837E|I'll Slap Your Fa...|ARSVTNL1187B992A91|       Jonathan King|2001|129.85424|\n",
      "|SOHOZBI12A8C132E3C|         Smash It Up|AR0MWD61187B9B2B12|International Noi...|2000|195.39546|\n",
      "+------------------+--------------------+------------------+--------------------+----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fu</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>Arellano</td>\n",
       "      <td>280.05832</td>\n",
       "      <td>free</td>\n",
       "      <td>Harrisburg-Carlisle, PA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540007e+12</td>\n",
       "      <td>514</td>\n",
       "      <td>Ja I Ty</td>\n",
       "      <td>200</td>\n",
       "      <td>1542069637796</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>66</td>\n",
       "      <td>2018-11-13 00:40:37.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All Time Low</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Maia</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Burke</td>\n",
       "      <td>177.84118</td>\n",
       "      <td>free</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540677e+12</td>\n",
       "      <td>510</td>\n",
       "      <td>A Party Song (The Walk of Shame)</td>\n",
       "      <td>200</td>\n",
       "      <td>1542071549796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebK...</td>\n",
       "      <td>51</td>\n",
       "      <td>2018-11-13 01:12:29.796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist       auth firstName gender  itemInSession  lastName  \\\n",
       "0            Fu  Logged In     Kevin      M              1  Arellano   \n",
       "1  All Time Low  Logged In      Maia      F              1     Burke   \n",
       "\n",
       "      length level                              location method      page  \\\n",
       "0  280.05832  free               Harrisburg-Carlisle, PA    PUT  NextSong   \n",
       "1  177.84118  free  Houston-The Woodlands-Sugar Land, TX    PUT  NextSong   \n",
       "\n",
       "   registration  sessionId                              song  status  \\\n",
       "0  1.540007e+12        514                           Ja I Ty     200   \n",
       "1  1.540677e+12        510  A Party Song (The Walk of Shame)     200   \n",
       "\n",
       "              ts                                          userAgent userId  \\\n",
       "0  1542069637796  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...     66   \n",
       "1  1542071549796  \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebK...     51   \n",
       "\n",
       "                timestamp  \n",
       "0 2018-11-13 00:40:37.796  \n",
       "1 2018-11-13 01:12:29.796  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Will join this two tables\n",
    "songs_table.show(2)\n",
    "\n",
    "#The other way is to retrieve from the parquet file\n",
    "#Local -> localdf=spark.read.parquet(\"song1.parquet\")\n",
    "#From S3 -> s3df=spark.read.parquet(output_data+ \"songs/\")\n",
    "\n",
    "\n",
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We will need to merge these 2 tables. So lets write a query\n",
    "df.createOrReplaceTempView(\"log_table\")\n",
    "songs_table.createOrReplaceTempView(\"song_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+\n",
      "|start_time|user_id|level|song_id|artist_id|session_id|location|user_agent|year|month|\n",
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+\n",
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays=spark.sql('''\n",
    "select  \n",
    "    l.timestamp as start_time,\n",
    "    l.userId as user_id,\n",
    "    l.level,\n",
    "    s.song_id,\n",
    "    s.artist_id,\n",
    "    l.sessionId as session_id,\n",
    "    l.location,\n",
    "    l.useragent as user_agent,\n",
    "    year(l.timestamp) as year,\n",
    "    month(l.timestamp) as month\n",
    "\n",
    "from \n",
    "    log_table as l \n",
    "inner join\n",
    "    song_table as s\n",
    "on \n",
    "    s.duration=l.length and s.title=l.song and s.artist_name=l.artist\n",
    "''')\n",
    "\n",
    "\n",
    "songplays.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
